{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"pose_model.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd /content/drive/MyDrive/EECS_442 Final Proj/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h2o3GdIar12s","executionInfo":{"status":"ok","timestamp":1650494636230,"user_tz":240,"elapsed":17104,"user":{"displayName":"yu","userId":"12477115984512032943"}},"outputId":"11f33076-06d5-43be-a0a9-f6a6268d0003"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/.shortcut-targets-by-id/1Sc7VSl-7PU4L0hxklxePzLVSFNQ7pnLE/EECS_442 Final Proj\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dky3MS5HkAbs"},"outputs":[],"source":["import os\n","import torch\n","import numpy as np\n","import pandas as pd\n","import math\n","import cv2\n","import json\n","import glob\n","import torchvision.transforms as transforms\n","from torch import nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import models as torch_models\n","from tqdm.auto import tqdm\n","from torchsummary import summary\n","import torch.optim as optim\n","from sklearn.metrics import accuracy_score"]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/EECS_442 Final Proj/WLASL/\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fTt1Yo_blVpR","executionInfo":{"status":"ok","timestamp":1650494643268,"user_tz":240,"elapsed":7,"user":{"displayName":"yu","userId":"12477115984512032943"}},"outputId":"9b17d2b8-b531-40f1-9774-de1e65e0ef53"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1Sc7VSl-7PU4L0hxklxePzLVSFNQ7pnLE/EECS_442 Final Proj/WLASL\n"]}]},{"cell_type":"code","source":["if torch.cuda.is_available():\n","    print(\"GPU\")\n","    device = 'cuda'\n","else:\n","    print(\"CPU\")\n","    device = 'cpu'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cgn6u_pjRik4","executionInfo":{"status":"ok","timestamp":1650494643268,"user_tz":240,"elapsed":5,"user":{"displayName":"yu","userId":"12477115984512032943"}},"outputId":"e732a2c9-633a-4300-c8a2-b5e3e018d176"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["GPU\n"]}]},{"cell_type":"markdown","source":["# Import Dataset"],"metadata":{"id":"IRCwah4SS-MI"}},{"cell_type":"code","source":["from torch.utils.data import Dataset\n","from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n","import random\n","\n","\n","def labels2cat(label_encoder, list):\n","    return label_encoder.transform(list)\n","\n","def compute_difference(x):\n","    diff = []\n","\n","    for i, xx in enumerate(x):\n","        temp = []\n","        for j, xxx in enumerate(x):\n","            if i != j:\n","                temp.append(xx - xxx)\n","\n","        diff.append(temp)\n","\n","    return diff\n","\n","\n","\n","class Sign_Dataset2(Dataset):\n","    def __init__(self, index_file_path, split, pose_root, sample_strategy='rnd_start', num_samples=25, num_copies=4,\n","                 img_transforms=None, video_transforms=None, test_index_file=None):\n","        assert os.path.exists(index_file_path), \"Non-existent indexing file path: {}.\".format(index_file_path)\n","        assert os.path.exists(pose_root), \"Path to poses does not exist: {}.\".format(pose_root)\n","\n","        self.data = []\n","        self.label_encoder, self.onehot_encoder = LabelEncoder(), OneHotEncoder(categories='auto')\n","\n","        if type(split) == 'str':\n","            split = [split]\n","\n","        self.test_index_file = test_index_file\n","        self._make_dataset(index_file_path, pose_root, split)\n","\n","        self.index_file_path = index_file_path\n","        self.pose_root = pose_root\n","        self.framename = 'image_{}_keypoints.json'\n","        self.sample_strategy = sample_strategy\n","        self.num_samples = num_samples\n","\n","        self.img_transforms = img_transforms\n","        self.video_transforms = video_transforms\n","\n","        self.num_copies = num_copies\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        video_id, gloss_cat, frame_start, frame_end = self.data[index]\n","        # frames of dimensions (T, H, W, C)\n","        x = self._load_poses(video_id, frame_start, frame_end, self.sample_strategy, self.num_samples)\n","\n","        if self.video_transforms:\n","            x = self.video_transforms(x)\n","\n","        y = gloss_cat\n","\n","        \n","\n","        return x, y, video_id\n","\n","    def _make_dataset(self, index_file_path, pose_root, split):\n","        with open(index_file_path, 'r') as f:\n","            content = json.load(f)\n","\n","        vid_ids = os.listdir(pose_root)\n","        \n","        glosses = sorted([gloss_entry['gloss'] for gloss_entry in content])\n","\n","        self.label_encoder.fit(glosses)\n","        self.onehot_encoder.fit(self.label_encoder.transform(self.label_encoder.classes_).reshape(-1, 1))\n","\n","        if self.test_index_file is not None:\n","            print('Trained on {}, tested on {}'.format(index_file_path, self.test_index_file))\n","            with open(self.test_index_file, 'r') as f:\n","                content = json.load(f)\n","\n","        # make dataset\n","        for gloss_entry in content:\n","            gloss, instances = gloss_entry['gloss'], gloss_entry['instances']\n","            gloss_cat = labels2cat(self.label_encoder, [gloss])[0]\n","\n","            for instance in instances:\n","                if instance['video_id'] not in vid_ids:\n","                    continue\n","                if instance['split'] not in split:\n","                    continue\n","\n","                frame_end = instance['frame_end']\n","                frame_start = instance['frame_start']\n","                video_id = instance['video_id']\n","\n","                instance_entry = video_id, gloss_cat, frame_start, frame_end\n","                self.data.append(instance_entry)\n","\n","    def _load_poses(self, video_id, frame_start, frame_end, sample_strategy, num_samples):\n","        \"\"\" Load frames of a video. Start and end indices are provided just to avoid listing and sorting the directory unnecessarily.\n","         \"\"\"\n","        poses = []\n","\n","        if sample_strategy == 'rnd_start':\n","            frames_to_sample = rand_start_sampling(frame_start, frame_end, num_samples)\n","\n","        else:\n","            raise NotImplementedError('Unimplemented sample strategy found: {}.'.format(sample_strategy))\n","\n","        \n","        for i in frames_to_sample:\n","            pose_path = os.path.join(self.pose_root, video_id, self.framename.format(str(i).zfill(5)))\n","            # pose = cv2.imread(frame_path, cv2.COLOR_BGR2RGB)\n","            pose = read_pose_file(pose_path)\n","            #print(pose.size())\n","\n","            if pose is not None:\n","                if self.img_transforms:\n","                    pose = self.img_transforms(pose)\n","\n","                poses.append(torch.flatten(pose))\n","            else:\n","                try:\n","                    poses.append(poses[-1])\n","                except Exception:\n","                    print(pose_path)\n","\n","        pad = None\n","\n","       \n","        # if len(frames_to_sample) < num_samples:\n","        if len(poses) < num_samples:\n","            num_padding = num_samples - len(frames_to_sample)\n","            last_pose = poses[-1]\n","            pad = last_pose.repeat(1, num_padding)\n","        \n","        # poses_across_time = torch.flatten(poses, dim=1)\n","\n","        # # poses_across_time = torch.cat(poses, dim=2)\n","        if pad is not None:\n","            poses = [poses, pad]\n","\n","        return torch.Tensor(poses)\n","\n","def read_pose_file(filepath):\n","    body_pose_exclude = {9, 10, 11, 22, 23, 24, 12, 13, 14, 19, 20, 21}\n","        \n","    path_parts = os.path.split(filepath)\n","\n","    frame_id = path_parts[1][:11]\n","    vid = os.path.split(path_parts[0])[-1]\n","\n","    save_to = os.path.join('./data/features', vid)\n","    \n","    try:\n","        ft = torch.load(os.path.join(save_to, frame_id + '_ft.pt'))\n","        xy = ft[:, :2]\n","  \n","        return xy\n","\n","    except FileNotFoundError:\n","\n","        try:\n","            #print(filepath)\n","          content = json.load(open(filepath))[\"people\"][0]\n","        except Exception:\n","          return None\n","        \n","        body_pose = content[\"pose_keypoints_2d\"]\n","        left_hand_pose = content[\"hand_left_keypoints_2d\"]\n","        right_hand_pose = content[\"hand_right_keypoints_2d\"]\n","    \n","        body_pose.extend(left_hand_pose)\n","        body_pose.extend(right_hand_pose)\n","    \n","        x = [v for i, v in enumerate(body_pose) if i % 3 == 0 and i // 3 not in body_pose_exclude]\n","        y = [v for i, v in enumerate(body_pose) if i % 3 == 1 and i // 3 not in body_pose_exclude]\n","        # conf = [v for i, v in enumerate(body_pose) if i % 3 == 2 and i // 3 not in body_pose_exclude]\n","    \n","        x = 2 * ((torch.FloatTensor(x) / 256.0) - 0.5)\n","        y = 2 * ((torch.FloatTensor(y) / 256.0) - 0.5)\n","        # conf = torch.FloatTensor(conf)\n","    \n","        x_diff = torch.FloatTensor(compute_difference(x)) / 2\n","        y_diff = torch.FloatTensor(compute_difference(y)) / 2\n","    \n","        zero_indices = (x_diff == 0).nonzero()\n","    \n","        orient = y_diff / x_diff\n","        orient[zero_indices] = 0\n","    \n","        xy = torch.stack([x, y]).transpose_(0, 1)\n","    \n","        ft = torch.cat([xy, x_diff, y_diff, orient], dim=1)\n","    \n","    \n","        xy = ft[:, :2]\n","        \n","        if not os.path.exists(save_to):\n","            os.mkdir(save_to)\n","        torch.save(ft, os.path.join(save_to, frame_id + '_ft.pt'))\n","        #print(\"Saving\",save_to, frame_id + '_ft.pt')\n","        return xy\n","\n","\n","def rand_start_sampling(frame_start, frame_end, num_samples):\n","    \"\"\"Randomly select a starting point and return the continuous ${num_samples} frames.\"\"\"\n","    num_frames = frame_end - frame_start + 1\n","\n","    if num_frames > num_samples:\n","        select_from = range(frame_start, frame_end - num_samples + 1)\n","        sample_start = random.choice(select_from)\n","        frames_to_sample = list(range(sample_start, sample_start + num_samples))\n","    else:\n","        frames_to_sample = list(range(frame_start, frame_end + 1))\n","\n","    return frames_to_sample\n"],"metadata":{"id":"xj8S2aSUP6xu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import Dataset\n","from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n","import random\n","\n","\n","def labels2cat(label_encoder, list):\n","    return label_encoder.transform(list)\n","\n","def compute_difference(x):\n","    diff = []\n","\n","    for i, xx in enumerate(x):\n","        temp = []\n","        for j, xxx in enumerate(x):\n","            if i != j:\n","                temp.append(xx - xxx)\n","\n","        diff.append(temp)\n","\n","    return diff\n","\n","\n","\n","class Sign_Dataset(Dataset):\n","    def __init__(self, index_file_path, split, pose_root, sample_strategy='rnd_start', num_samples=25, num_copies=4,\n","                 img_transforms=None, video_transforms=None, test_index_file=None):\n","        assert os.path.exists(index_file_path), \"Non-existent indexing file path: {}.\".format(index_file_path)\n","        assert os.path.exists(pose_root), \"Path to poses does not exist: {}.\".format(pose_root)\n","\n","        self.data = []\n","        self.label_encoder, self.onehot_encoder = LabelEncoder(), OneHotEncoder(categories='auto')\n","\n","        if type(split) == 'str':\n","            split = [split]\n","\n","        self.test_index_file = test_index_file\n","        self._make_dataset(index_file_path, pose_root, split)\n","\n","        self.index_file_path = index_file_path\n","        self.pose_root = pose_root\n","        self.framename = 'image_{}_keypoints.json'\n","        self.sample_strategy = sample_strategy\n","        self.num_samples = num_samples\n","\n","        self.img_transforms = img_transforms\n","        self.video_transforms = video_transforms\n","\n","        self.num_copies = num_copies\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        video_id, gloss_cat, frame_start, frame_end = self.data[index]\n","        # frames of dimensions (T, H, W, C)\n","        x = self._load_poses(video_id, frame_start, frame_end, self.sample_strategy, self.num_samples)\n","\n","        if self.video_transforms:\n","            x = self.video_transforms(x)\n","\n","        y = gloss_cat\n","\n","        \n","\n","        return x, y, video_id\n","\n","    def _make_dataset(self, index_file_path, pose_root, split):\n","        with open(index_file_path, 'r') as f:\n","            content = json.load(f)\n","\n","        vid_ids = os.listdir(pose_root)\n","        \n","        glosses = sorted([gloss_entry['gloss'] for gloss_entry in content])\n","\n","        self.label_encoder.fit(glosses)\n","        self.onehot_encoder.fit(self.label_encoder.transform(self.label_encoder.classes_).reshape(-1, 1))\n","\n","        if self.test_index_file is not None:\n","            print('Trained on {}, tested on {}'.format(index_file_path, self.test_index_file))\n","            with open(self.test_index_file, 'r') as f:\n","                content = json.load(f)\n","\n","        # make dataset\n","        for gloss_entry in content:\n","            gloss, instances = gloss_entry['gloss'], gloss_entry['instances']\n","            gloss_cat = labels2cat(self.label_encoder, [gloss])[0]\n","\n","            for instance in instances:\n","                if instance['video_id'] not in vid_ids:\n","                    continue\n","                if instance['split'] not in split:\n","                    continue\n","\n","                frame_end = instance['frame_end']\n","                frame_start = instance['frame_start']\n","                video_id = instance['video_id']\n","\n","                instance_entry = video_id, gloss_cat, frame_start, frame_end\n","                self.data.append(instance_entry)\n","\n","    def _load_poses(self, video_id, frame_start, frame_end, sample_strategy, num_samples):\n","        \"\"\" Load frames of a video. Start and end indices are provided just to avoid listing and sorting the directory unnecessarily.\n","         \"\"\"\n","        poses = []\n","\n","        if sample_strategy == 'rnd_start':\n","            frames_to_sample = rand_start_sampling(frame_start, frame_end, num_samples)\n","\n","        else:\n","            raise NotImplementedError('Unimplemented sample strategy found: {}.'.format(sample_strategy))\n","\n","        \n","        for i in frames_to_sample:\n","            pose_path = os.path.join(self.pose_root, video_id, self.framename.format(str(i).zfill(5)))\n","            # pose = cv2.imread(frame_path, cv2.COLOR_BGR2RGB)\n","            pose = read_pose_file(pose_path)\n","            #print(pose.size())\n","\n","            if pose is not None:\n","                if self.img_transforms:\n","                    pose = self.img_transforms(pose)\n","\n","                poses.append(pose)\n","            else:\n","                try:\n","                    poses.append(poses[-1])\n","                except Exception:\n","                    print(pose_path)\n","\n","        pad = None\n","\n","        # if len(frames_to_sample) < num_samples:\n","        if len(poses) < num_samples:\n","            num_padding = num_samples - len(frames_to_sample)\n","            last_pose = poses[-1]\n","            pad = last_pose.repeat(1, num_padding)\n","\n","        poses_across_time = torch.cat(poses, dim=1)\n","        if pad is not None:\n","            poses_across_time = torch.cat([poses_across_time, pad], dim=1)\n","\n","        return poses_across_time\n","\n","def read_pose_file(filepath):\n","    body_pose_exclude = {9, 10, 11, 22, 23, 24, 12, 13, 14, 19, 20, 21}\n","        \n","    path_parts = os.path.split(filepath)\n","\n","    frame_id = path_parts[1][:11]\n","    vid = os.path.split(path_parts[0])[-1]\n","\n","    save_to = os.path.join('./data/features', vid)\n","    \n","    try:\n","        ft = torch.load(os.path.join(save_to, frame_id + '_ft.pt'))\n","        xy = ft[:, :2]\n","  \n","        return xy\n","\n","    except FileNotFoundError:\n","\n","        try:\n","            #print(filepath)\n","          content = json.load(open(filepath))[\"people\"][0]\n","        except Exception:\n","          return None\n","        \n","        body_pose = content[\"pose_keypoints_2d\"]\n","        left_hand_pose = content[\"hand_left_keypoints_2d\"]\n","        right_hand_pose = content[\"hand_right_keypoints_2d\"]\n","    \n","        body_pose.extend(left_hand_pose)\n","        body_pose.extend(right_hand_pose)\n","    \n","        x = [v for i, v in enumerate(body_pose) if i % 3 == 0 and i // 3 not in body_pose_exclude]\n","        y = [v for i, v in enumerate(body_pose) if i % 3 == 1 and i // 3 not in body_pose_exclude]\n","        # conf = [v for i, v in enumerate(body_pose) if i % 3 == 2 and i // 3 not in body_pose_exclude]\n","    \n","        x = 2 * ((torch.FloatTensor(x) / 256.0) - 0.5)\n","        y = 2 * ((torch.FloatTensor(y) / 256.0) - 0.5)\n","        # conf = torch.FloatTensor(conf)\n","    \n","        x_diff = torch.FloatTensor(compute_difference(x)) / 2\n","        y_diff = torch.FloatTensor(compute_difference(y)) / 2\n","    \n","        zero_indices = (x_diff == 0).nonzero()\n","    \n","        orient = y_diff / x_diff\n","        orient[zero_indices] = 0\n","    \n","        xy = torch.stack([x, y]).transpose_(0, 1)\n","    \n","        ft = torch.cat([xy, x_diff, y_diff, orient], dim=1)\n","    \n","    \n","        xy = ft[:, :2]\n","        \n","        if not os.path.exists(save_to):\n","            os.mkdir(save_to)\n","        torch.save(ft, os.path.join(save_to, frame_id + '_ft.pt'))\n","        #print(\"Saving\",save_to, frame_id + '_ft.pt')\n","        return xy\n","\n","\n","def rand_start_sampling(frame_start, frame_end, num_samples):\n","    \"\"\"Randomly select a starting point and return the continuous ${num_samples} frames.\"\"\"\n","    num_frames = frame_end - frame_start + 1\n","\n","    if num_frames > num_samples:\n","        select_from = range(frame_start, frame_end - num_samples + 1)\n","        sample_start = random.choice(select_from)\n","        frames_to_sample = list(range(sample_start, sample_start + num_samples))\n","    else:\n","        frames_to_sample = list(range(frame_start, frame_end + 1))\n","\n","    return frames_to_sample\n"],"metadata":{"id":"tMZH-lmpOlWu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training Split"],"metadata":{"id":"D-Wa26NaTGMM"}},{"cell_type":"code","source":["split_file = './data/splits/asl100.json'\n","keypoint_dir = './data/pose_per_individual_videos/'\n","num_samples = 50\n","\n","train_dataset = Sign_Dataset(index_file_path=split_file, split='train', pose_root=keypoint_dir,\n","                                img_transforms=None, video_transforms=None, num_samples=num_samples)\n","val_dataset = Sign_Dataset(index_file_path=split_file, split='val', pose_root=keypoint_dir,\n","                                img_transforms=None, video_transforms=None, num_samples=num_samples)\n","test_dataset = Sign_Dataset(index_file_path=split_file, split='test', pose_root=keypoint_dir,\n","                                img_transforms=None, video_transforms=None, num_samples=num_samples)\n","\n","# train_dataset = Sign_Dataset2(index_file_path=split_file, split='train', pose_root=keypoint_dir,\n","#                                 img_transforms=None, video_transforms=None, num_samples=num_samples)\n","# val_dataset = Sign_Dataset2(index_file_path=split_file, split='val', pose_root=keypoint_dir,\n","#                                 img_transforms=None, video_transforms=None, num_samples=num_samples)\n","# test_dataset = Sign_Dataset2(index_file_path=split_file, split='test', pose_root=keypoint_dir,\n","#                                 img_transforms=None, video_transforms=None, num_samples=num_samples)"],"metadata":{"id":"vYb-PqR6mH1r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset[2][0].size()"],"metadata":{"id":"4V7oUuiFHmfg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650494703465,"user_tz":240,"elapsed":19054,"user":{"displayName":"yu","userId":"12477115984512032943"}},"outputId":"a48e03c5-6797-46b2-f4d4-bdd6f478ef9b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([55, 100])"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["batch_size = 32\n","\n","train_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True, num_workers = 4, pin_memory= 4)\n","val_loader = DataLoader(val_dataset,batch_size=batch_size,shuffle=True, num_workers = 4, pin_memory= 4)\n","test_loader = DataLoader(test_dataset,batch_size=batch_size,shuffle=True, num_workers = 4, pin_memory= 4)"],"metadata":{"id":"-YvRuZPsQ3Qe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650494703465,"user_tz":240,"elapsed":8,"user":{"displayName":"yu","userId":"12477115984512032943"}},"outputId":"a286ca52-e0a5-4950-8dc6-48632614767b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]}]},{"cell_type":"code","source":["NUM_CLASSES = len(train_dataset.label_encoder.classes_)\n","NUM_FEATURES = 100"],"metadata":{"id":"8XXhk8-Gj22Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","```\n","# This is formatted as code\n","```\n","\n","# My Models"],"metadata":{"id":"8UPuiS0NTM6I"}},{"cell_type":"code","source":["class PoseGRU(nn.Module):\n","  def __init__(self,input_size=100,hidden_size = 128,output_size = 100, num_layers =2, drop = 0.0):\n","    super().__init__()\n","    self.input_size = input_size\n","    self.hidden_size = hidden_size\n","    self.output_size = output_size\n","    self.num_layers = num_layers\n","    self.drop = drop\n","\n","    self.gru1 = nn.GRU(input_size = self.input_size, hidden_size = 64, num_layers = self.num_layers, batch_first = True)\n","    self.gru2 = nn.GRU(64, hidden_size = 64, num_layers = self.num_layers, batch_first = True)\n","    self.gru3 = nn.GRU(64, hidden_size = 128, num_layers = self.num_layers, batch_first = True)\n","    self.gru4 = nn.GRU(128, hidden_size = 128, num_layers = self.num_layers, batch_first = True)\n","\n","    self.dropout = nn.Dropout(p=self.drop)\n","    self.fc1 = nn.Linear(self.hidden_size,self.input_size//2)\n","    self.fc2 = nn.Linear(self.input_size//2,self.output_size)\n","    self.relu = nn.ReLU()\n","    self.softmax = nn.Softmax(dim=-1)\n","\n","  def forward(self, x):\n","    print(\"1\")\n","    output,h1 = self.gru1(x,None)\n","    print(\"1\")\n","    output,h2 = self.gru2(output,h1)\n","    print(\"1\")\n","    output,h3 = self.gru3(output,h2)\n","    print(\"1\")\n","    output,h = self.gru4(output,h3)\n","    print(\"1\")\n","    output = self.dropout(output)\n","\n","    output = self.fc1(output[:,-1,:])\n","    return self.softmax(self.fc2(output))\n"],"metadata":{"id":"z4WhC75RPyDM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class PoseGRU2(nn.Module):\n","  def __init__(self,input_size=55,hidden_size = 128,output_size = 100, num_layers =2, drop = 0.0):\n","    super().__init__()\n","    self.input_size = input_size\n","    self.hidden_size = hidden_size\n","    self.output_size = output_size\n","    self.num_layers = num_layers\n","    self.drop = drop\n","\n","    self.lstm = nn.GRU(input_size = self.input_size, hidden_size = self.hidden_size, num_layers = self.num_layers, batch_first = True)\n","    self.dropout = nn.Dropout(p=self.drop)\n","    self.fc1 = nn.Linear(self.hidden_size*2,self.output_size)\n","\n","    self.relu = nn.ReLU()\n","\n","  def forward(self, x):\n","    output,self.h = self.lstm(x,None)\n","    avg_pool = F.adaptive_avg_pool1d(output,1)\n","    max_pool = F.adaptive_max_pool1d(output,1)\n","    print(avg_pool.size(),max_pool.size())\n","    print(\"here\")\n","    outp = self.fc1(torch.cat([avg_pool,max_pool],dim=0))             \n","    return F.softmax(outp, dim=-1) \n"],"metadata":{"id":"hKQODQ4uy7Em"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"KRKvW1POTOUF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Train\n"],"metadata":{"id":"JsfiHFGPTSQF"}},{"cell_type":"code","source":["model = PoseGRU(drop = 0.3,input_size=100,output_size=NUM_CLASSES).to(device)\n","#model2 = PoseGRU2(drop = 0.3,input_size=100,output_size=NUM_CLASSES).to(device)\n","#model3 = LSTM(drop_p = 0.3, lstm_input_size=110,num_classes=100).to(device)\n","\n","summary(model, (50,110), device=device)"],"metadata":{"id":"9EhafS9hSmr9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"FQu4fY4KT6V-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x = torch.ones([2, 55, 100]).to(device)\n","model(x).size()"],"metadata":{"id":"1N7b1-7wYnbs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["EPOCHS = 200\n","\n","LR = 0.001\n","EPS = 1e-3\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=LR, eps = EPS)"],"metadata":{"id":"BdcMZBDPW_1-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate(model, loader):  # Evaluate accuracy on validation / test set\n","    model.eval()  # Set the model to evaluation mode\n","    correct = 0\n","    with torch.no_grad():  # Do not calculate grident to speed up computation\n","        for batch, (pose,y,id) in tqdm(loader):\n","            pose,y,id = pose.to(device),y.to(device).view(-1,),id\n","            correct += (torch.argmax(pred, dim=1) == y).sum().item()\n","        acc = correct/len(loader.dataset)\n","        print(\"\\n Evaluation accuracy: {}\".format(acc))\n","        return acc"],"metadata":{"id":"fxJ120cc6N9V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["all_labels = []\n","all_preds = []\n","for e in range(EPOCHS):\n","    tot_loss = 0.0\n","    tot_loss2 = 0.0\n","    labels = []\n","    preds = []\n","    count = 0\n","\n","    model.train()\n","    for batch,data in enumerate(train_loader):\n","        pose,y,id = data[0].to(device),data[1].to(device).view(-1,),data[2]\n","\n","        # print(pose.size())\n","        # print(y)\n","        optimizer.zero_grad()\n","        print(pose.size())\n","        out = model(pose)\n","        # out2 = model2(pose)\n","\n","        loss = criterion(out, y)\n","        #loss2 = criterion(out2, y)\n","        tot_loss += loss\n","        #tot_loss2 += loss\n","\n","        # pred = torch.argmax(out, dim=1)\n","        pred = torch.max(out, 1)[1]\n","\n","       # print(pred,y)\n","\n","        labels.extend(y.cpu().data.squeeze().tolist())\n","        preds.extend(pred.cpu().data.squeeze().tolist())\n","        \n","        acc = accuracy_score(y.cpu().data.squeeze().tolist(),pred.cpu().data.squeeze().tolist())\n","        # acc2 = accuracy_score(y.cpu().data.squeeze().tolist(),pred2.cpu().data.squeeze().tolist())\n","\n","        print(\"Batch:{} Accuracy :{}\".format(batch,acc))\n","\n","        count +=1\n","\n","        loss.backward()\n","        # loss2.backward()\n","        optimizer.step()\n","\n","    #evaluate(model,val_loader)\n","    accuracy = accuracy_score(labels,preds)\n","    all_labels.extend(labels)\n","    all_preds.extend(preds)\n","    print(\"\\nEpoch:{} Training loss:{} Training Accuracy:{}\".format(e+1,tot_loss/count,accuracy))\n","\n","\n","\n","\n","\n","  \n","\n","\n","\n","\n"],"metadata":{"id":"_8x2jzYwpOsx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"mAcPKBA94uew"},"execution_count":null,"outputs":[]}]}